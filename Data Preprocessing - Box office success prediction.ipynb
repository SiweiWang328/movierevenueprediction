{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# read data\n",
    "test = pd.read_csv(\"data/test.csv\")\n",
    "train = pd.read_csv(\"data/train.csv\")\n",
    "\n",
    "# combine test and train\n",
    "all_data = pd.concat([train, test], axis = 0)\n",
    "all_data.isnull().sum()\n",
    "\n",
    "# preprosess feature - belongs_to_collection\n",
    "def process_belongs_to_collection(variable):\n",
    "    \n",
    "    num_null = variable.isnull().sum()\n",
    "    \n",
    "    if num_null > 0:\n",
    "        variable = variable.fillna(0)\n",
    "    \n",
    "    else: \n",
    "        variable = variable\n",
    "        \n",
    "    variable_l = variable.tolist()\n",
    "        \n",
    "    variable_nm = list(map(lambda x: str(x).split(', ')[1].partition(\"'name': '\")[2].replace(\"'\", '') \n",
    "                         if x != 0 else 0, variable_l))\n",
    "        \n",
    "    all_data['collection_nm'] = variable_nm\n",
    "    \n",
    "    all_data['collection_nm'].loc[all_data['collection_nm'] != 0] = 1\n",
    "    \n",
    "    print (all_data['collection_nm'].head())\n",
    "\n",
    "# drop belongs_to_collection\n",
    "all_data_1 = all_data.drop('belongs_to_collection', axis=1)\n",
    "\n",
    "# preprosess feature - genres\n",
    "def process_genres(variable):\n",
    "\n",
    "    try:\n",
    "        d = eval(variable)\n",
    "    except:\n",
    "        d = {}\n",
    "    return d\n",
    "\n",
    "all_data_1 = all_data_1\n",
    "all_data_1['genres'] = all_data_1['genres'].map(lambda x: \n",
    "                            sorted([d['name'] for d in process_genres(x)])).map(lambda x: ','.join(map(str, x)))\n",
    "genres = all_data_1.genres.str.get_dummies(sep=',')\n",
    "all_data_1 = pd.concat([all_data_1, genres], axis=1, sort=False)\n",
    "print(\"Action Genres Movie           \", all_data_1[all_data_1.Action == 1].shape[0])\n",
    "print(\"Adventure Genres Movie        \", all_data_1[all_data_1.Adventure == 1].shape[0])\n",
    "print(\"Animation Genres Movie        \", all_data_1[all_data_1.Animation == 1].shape[0])\n",
    "print(\"Comedy Genres Movie           \", all_data_1[all_data_1.Comedy == 1].shape[0])\n",
    "print(\"Crime Genres Movie            \", all_data_1[all_data_1.Crime == 1].shape[0])\n",
    "print(\"Documentary Genres Movie      \", all_data_1[all_data_1.Documentary == 1].shape[0])\n",
    "print(\"Drama Genres Movie            \", all_data_1[all_data_1.Drama == 1].shape[0])\n",
    "print(\"Family Genres Movie           \", all_data_1[all_data_1.Family == 1].shape[0])\n",
    "print(\"Fantasy Genres Movie          \", all_data_1[all_data_1.Fantasy == 1].shape[0])\n",
    "print(\"Foreign Genres Movie          \", all_data_1[all_data_1.Foreign == 1].shape[0])\n",
    "print(\"History Genres Movie          \", all_data_1[all_data_1.History == 1].shape[0])\n",
    "print(\"Music Genres Movie            \", all_data_1[all_data_1.Music == 1].shape[0])\n",
    "print(\"Mystery Genres Movie          \", all_data_1[all_data_1.Mystery == 1].shape[0])\n",
    "print(\"Romance Genres Movie          \", all_data_1[all_data_1.Romance == 1].shape[0])\n",
    "print(\"Science Fiction Genres Movie  \", all_data_1[all_data_1['Science Fiction'] == 1].shape[0])\n",
    "print(\"TV Movie Genres Movie         \", all_data_1[all_data_1['TV Movie'] == 1].shape[0])\n",
    "print(\"Thriller Genres Movie         \", all_data_1[all_data_1.Thriller == 1].shape[0])\n",
    "print(\"War Genres Movie              \", all_data_1[all_data_1.War == 1].shape[0])\n",
    "print(\"Western Genres Movie          \", all_data_1[all_data_1.Western == 1].shape[0])\n",
    "\n",
    "# drop genres\n",
    "all_data_1 = all_data_1.drop('genres', axis=1)\n",
    "\n",
    "# preprosess feature - homepage\n",
    "def process_homepage(variable):\n",
    "    \n",
    "    variable.loc[~variable.isnull()] = 1\n",
    "    \n",
    "    all_data_1[\"homepage\"] = variable.fillna(0)\n",
    "    \n",
    "    print (all_data_1[\"homepage\"].head())\n",
    "\n",
    "# preprocess feature - release_date\n",
    "def process_release_date(variable):\n",
    "    \n",
    "    all_data_1[['release_month','release_day','release_year']]= variable.str.split('/',expand=True).replace(np.nan, -1).astype(int)\n",
    "    \n",
    "    # Some rows have 4 digits of year instead of 2, that's why I am applying (train['release_year'] < 100) this condition\n",
    "    all_data_1.loc[ (all_data_1['release_year'] <= 18) & (all_data_1['release_year'] < 100), \"release_year\"] += 2000\n",
    "    all_data_1.loc[ (all_data_1['release_year'] > 18)  & (all_data_1['release_year'] < 100), \"release_year\"] += 1900\n",
    "    \n",
    "    releaseDate = pd.to_datetime(variable) \n",
    "    all_data_1['release_dayofweek'] = releaseDate.dt.dayofweek\n",
    "    all_data_1['release_quarter'] = releaseDate.dt.quarter\n",
    "    \n",
    "    print(all_data_1[['release_month','release_day','release_year', 'release_dayofweek', 'release_quarter']].head())\n",
    "\n",
    "all_data_1 = all_data_1.drop('release_date', axis=1)\n",
    "\n",
    "# preprocess feature - budget\n",
    "def process_budget(variable):\n",
    "    \n",
    "    all_data_1['budget_log'] = np.log1p(variable)\n",
    "    \n",
    "    ### budget runtime ratio\n",
    "    all_data_1[\"budget_runtime_ratio\"] = all_data_1['budget_log']/all_data_1['runtime'] \n",
    "    \n",
    "    ### budget popularity ratio\n",
    "    all_data_1['budget_popularity_ratio'] = all_data_1['budget_log']/all_data_1['popularity']\n",
    "    \n",
    "    ### budget year ratio\n",
    "    all_data_1['budget_year_ratio'] = all_data_1['budget_log']/(all_data_1['release_year']*all_data_1['release_year'])\n",
    "\n",
    "    print(all_data_1[['budget_log',\"budget_runtime_ratio\", \n",
    "                  'budget_popularity_ratio', \n",
    "                 'budget_year_ratio']].head())\n",
    "\n",
    "# mean budget by year\n",
    "meanBudgetByYear = all_data_1.groupby('release_year')['budget_log'].aggregate('mean')\n",
    "all_data_1 = pd.merge(all_data_1, pd.DataFrame(meanBudgetByYear), on = 'release_year', how='left')\n",
    "\n",
    "all_data_1 = all_data_1.drop('budget', axis=1)\n",
    "\n",
    "# preprocess feature - original language\n",
    "def process_original_language(variable):\n",
    "    \n",
    "    all_data_1.loc[variable =='en', 'original_language'] = 1\n",
    "    \n",
    "    all_data_1.loc[variable !=1, 'original_language'] = 0\n",
    "    \n",
    "    print(all_data_1['original_language'].head())\n",
    "\n",
    "all_data_1 = all_data_1.drop('original language', axis=1)\n",
    "\n",
    "# preprocess feature - title, original_title\n",
    "def process_original_title(variable_1, variable_2):\n",
    "    \n",
    "    all_data_1['isTitleDifferent'] = 1\n",
    "\n",
    "    all_data_1.loc[variable_2 == variable_1 ,\"isTitleDifferent\"] = 0 \n",
    "    \n",
    "    all_data_1['original_title_letter_count'] = variable_2.str.len() \n",
    "    \n",
    "    all_data_1['original_title_word_count'] = variable_2.str.split().str.len() \n",
    "\n",
    "    all_data_1['title_word_count'] = variable_1.str.split().str.len()\n",
    "    \n",
    "    print (all_data_1[['isTitleDifferent', 'original_title_letter_count', \n",
    "                   'original_title_word_count', 'title_word_count' ]].head())\n",
    "\n",
    "all_data_1 = all_data_1.drop(['title', 'original_title'], axis=1)\n",
    "\n",
    "## preprocess feature - overview (Textual data)\n",
    "import spacy\n",
    "from gensim.parsing.preprocessing import *\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "import nltk\n",
    "import string\n",
    "from gensim.utils import simple_preprocess\n",
    "from tqdm import tqdm\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import wordnet\n",
    "from wordcloud import WordCloud\n",
    "from gensim import models\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import metrics\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction import text\n",
    "\n",
    "def process_overview_tfidf(variable):\n",
    "    \n",
    "    #replace null value\n",
    "    num_null = variable.isnull().sum()\n",
    "    \n",
    "    if num_null > 0:\n",
    "        variable = variable.fillna('NaN')\n",
    "    \n",
    "    else: \n",
    "        variable = variable\n",
    "        \n",
    "    overview = variable.tolist()\n",
    "    \n",
    "    #tfidf\n",
    "    my_additional_stop_words = ['year', 'story', 'world', 'time', 'film', 'day', \n",
    "                                'life', 'man', 'movie', 'set', 'lives', 'makes', \n",
    "                                'named', 'people', 'things', 'tries', 'trying', \n",
    "                               'turn', 'äì', 'äôs']\n",
    "    stop_words = text.ENGLISH_STOP_WORDS.union(my_additional_stop_words)\n",
    "    \n",
    "    tfidf_vect = TfidfVectorizer(stop_words=stop_words, \n",
    "                             max_df = 0.9,\n",
    "                             min_df=0.015) \n",
    "    \n",
    "    dtm_all_data_1= tfidf_vect.fit_transform(overview)\n",
    "    \n",
    "    return dtm_all_data_1, tfidf_vect\n",
    "\n",
    "dtm_all_data_1, tfidf_vect = process_overview_tfidf(all_data_1['overview'])\n",
    "\n",
    "dtm_all_data_1 = pd.DataFrame(dtm_all_data_1.toarray(), columns = tfidf_vect.get_feature_names())\n",
    "\n",
    "all_data_3 = pd.concat([all_data_1.reset_index(drop=True), dtm_all_data_1], axis = 1)\n",
    "\n",
    "all_data_3 = all_data_3.drop(['overview'], axis=1)\n",
    "\n",
    "# preprocess feature - popularity\n",
    "def process_popularity(variable):\n",
    "    \n",
    "    all_data_3['_releaseYear_popularity_ratio'] = all_data_3['release_year']/variable\n",
    "    \n",
    "    all_data_3['_releaseYear_popularity_ratio2'] = variable/all_data_3['release_year']\n",
    "    \n",
    "    print (all_data_3[['_releaseYear_popularity_ratio', '_releaseYear_popularity_ratio2']].head()) \n",
    "\n",
    "release_year_popularity_mean = all_data_3.groupby(\"release_year\")[\"popularity\"].aggregate('mean')\n",
    "release_year_popularity_mean = pd.DataFrame(release_year_popularity_mean)\n",
    "all_data_3 = pd.merge(all_data_3, release_year_popularity_mean, on = 'release_year', how='left')\n",
    "\n",
    "# preprocess feature - production_companies\n",
    "import ast\n",
    "def parse_company(x):\n",
    "    try:\n",
    "        results = ast.literal_eval(x)\n",
    "    except:\n",
    "        results = []\n",
    "    return results\n",
    " \n",
    "def process_production_companies(variable):\n",
    "    \n",
    "    num_null = variable.isnull().sum()\n",
    "    \n",
    "    if num_null > 0:\n",
    "        variable = variable.fillna(0)\n",
    "    \n",
    "    else: \n",
    "        variable = variable\n",
    "        \n",
    "    companies = variable.map(lambda x: parse_company(x))\n",
    "    \n",
    "    all_data_3['production_companies_count'] = companies.apply(lambda x : len(x))\n",
    "\n",
    "    print (all_data_3['production_companies_count'].head())\n",
    "    \n",
    "all_data_3 = all_data_3.drop('production_companies', axis=1)\n",
    "\n",
    "# preprocess feature - production_countries\n",
    "def parse_country(x):\n",
    "    try:\n",
    "        results = ast.literal_eval(x)\n",
    "    except:\n",
    "        results = []\n",
    "    return results\n",
    "\n",
    "def process_production_countries(variable):\n",
    "    \n",
    "    num_null = variable.isnull().sum()\n",
    "    \n",
    "    if num_null > 0:\n",
    "        variable = variable.fillna('NaN')\n",
    "    \n",
    "    else: \n",
    "        variable = variable\n",
    "    \n",
    "    countries = variable.map(lambda x: parse_country(x))\n",
    "    \n",
    "    countries_2 = countries.apply(lambda x: [i['name'] for i in x] if x!={} else []).values\n",
    "    \n",
    "    return countries, countries_2\n",
    " \n",
    "# convert countries into dummy variables\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "countries_dummy = pd.DataFrame(mlb.fit_transform(all_data_3['countries']),\n",
    "                               columns=mlb.classes_, index=all_data_3.index)\n",
    "\n",
    "all_data_3 = pd.concat([all_data_3, countries_dummy], axis=1)\n",
    "all_data_3 = all_data_3.drop(['countries'], axis=1)\n",
    "\n",
    "# preprocess feature - spoken_language\n",
    "def parse_lang(x):\n",
    "    try:\n",
    "        results = ast.literal_eval(x)\n",
    "    except:\n",
    "        results = []\n",
    "    return results\n",
    "\n",
    "def process_spoken_language(variable):\n",
    "   \n",
    "    num_null = variable.isnull().sum()\n",
    "    \n",
    "    if num_null > 0:\n",
    "        variable = variable.fillna('NaN')\n",
    "    \n",
    "    else: \n",
    "        variable = variable\n",
    "    \n",
    "    lang = variable.map(lambda x: parse_lang(x))\n",
    "    \n",
    "    lang = lang.apply(lambda x: [i['name'] for i in x] if x!={} else []).values\n",
    "    \n",
    "    all_data_3['lang'] = lang\n",
    "    \n",
    "    mlb = MultiLabelBinarizer()\n",
    "    \n",
    "    lang_dummy = pd.DataFrame(mlb.fit_transform(all_data_3['lang']),\n",
    "                                   columns=mlb.classes_, index=all_data_3.index)\n",
    "    \n",
    "    return lang_dummy\n",
    "\n",
    "all_data_3 = pd.concat([all_data_3, lang_dummy], axis=1)\n",
    "all_data_3 = all_data_3.drop('spoken_languages', axis = 1)\n",
    "\n",
    "# preprocess feature - tagline\n",
    "def process_tagline(variable):\n",
    "    \n",
    "    all_data_3['isTaglineNA'] = 0\n",
    "    \n",
    "    all_data_3.loc[variable == 0 ,\"isTaglineNA\"] = 1 \n",
    "    \n",
    "    all_data_3['tagline_word_count'] = variable.str.split().str.len()\n",
    "    \n",
    "    return all_data_3['isTaglineNA'], all_data_3['tagline_word_count']\n",
    "    \n",
    "all_data_3['isTaglineNA'], all_data_3['tagline_word_count'] = process_tagline(all_data_3['tagline'])\n",
    "\n",
    "## preprocess feature - keywords (Textual variable)\n",
    "def parse_keywords(x):\n",
    "    try:\n",
    "        results = ast.literal_eval(x)\n",
    "    except:\n",
    "        results = []\n",
    "    return results\n",
    "\n",
    "def process_keywords(variable):\n",
    "    \n",
    "    num_null = variable.isnull().sum()\n",
    "    \n",
    "    if num_null > 0:\n",
    "        variable = variable.fillna('NaN')\n",
    "    \n",
    "    else: \n",
    "        variable = variable\n",
    "    \n",
    "    keywords = variable.map(lambda x: parse_keywords(x))\n",
    "    \n",
    "    keywords=keywords.apply(lambda x: [i['name'] for i in x] if x!={} else []).values\n",
    "    \n",
    "    keywords_l = keywords.tolist()\n",
    "    \n",
    "    keywords_ll = [[' '.join(i)] for i in keywords_l]\n",
    "    \n",
    "    keywords_flat_list = [item for sublist in keywords_ll for item in sublist]\n",
    "    \n",
    "    #generate tfidf vector\n",
    "    tfidf_vect_2 = TfidfVectorizer(stop_words=None, min_df=50) \n",
    "    \n",
    "    dtm_2= tfidf_vect_2.fit_transform(keywords_flat_list)\n",
    "    \n",
    "    dtm_keywords = pd.DataFrame(dtm_2.toarray(), columns = tfidf_vect_2.get_feature_names())\n",
    "    \n",
    "    return dtm_keywords, tfidf_vect_2\n",
    "\n",
    "dtm_keywords, tfidf_vect_2 = process_keywords(all_data_3['Keywords'])\n",
    "\n",
    "all_data_3 = pd.concat([all_data_3, dtm_keywords], axis = 1)\n",
    "all_data_3 = all_data_3.drop('Keywords', axis = 1)\n",
    "\n",
    "# preprocess feature - cast gender\n",
    "def parse_cast(x):\n",
    "    try:\n",
    "        results = ast.literal_eval(x)\n",
    "    except:\n",
    "        results = []\n",
    "    return results\n",
    "\n",
    "def process_cast_gender(variable):\n",
    "    \n",
    "    num_null = variable.isnull().sum()\n",
    "    \n",
    "    if num_null > 0:\n",
    "        variable = variable.fillna('NaN')\n",
    "    \n",
    "    else: \n",
    "        variable = variable\n",
    "    \n",
    "    cast = variable.map(lambda x: parse_cast(x))\n",
    "    \n",
    "    all_data_3['genders_0_cast'] = cast.apply(lambda x: sum([1 for i in x if i['gender'] == 0]))\n",
    "    all_data_3['genders_1_cast'] = cast.apply(lambda x: sum([1 for i in x if i['gender'] == 1]))\n",
    "    all_data_3['genders_2_cast'] = cast.apply(lambda x: sum([1 for i in x if i['gender'] == 2]))\n",
    "    \n",
    "    return all_data_3['genders_0_cast'], all_data_3['genders_1_cast'], all_data_3['genders_2_cast']\n",
    "\n",
    "all_data_3['genders_0_cast'], \\\n",
    "    all_data_3['genders_1_cast'], all_data_3['genders_2_cast'] = process_cast_gender(all_data_3['cast'])\n",
    "\n",
    "# Preprocess feature - cast name\n",
    "def process_cast_name(variable):\n",
    "    \n",
    "    num_null = variable.isnull().sum()\n",
    "    \n",
    "    if num_null > 0:\n",
    "        variable = variable.fillna('NaN')\n",
    "    \n",
    "    else: \n",
    "        variable = variable\n",
    "    \n",
    "    cast = variable.map(lambda x: parse_cast(x))\n",
    "    \n",
    "    cast_name=cast.apply(lambda x: [i['name'] for i in x] if x!={} else []).values\n",
    "    \n",
    "    cast_name_l = cast_name.tolist()\n",
    "    \n",
    "    all_data_3['cast_name'] = cast_name_l\n",
    "    \n",
    "    mlb = MultiLabelBinarizer()\n",
    "    \n",
    "    #all_data_2 = all_data_1.join(pd.DataFrame(mlb.fit_transform(all_data_1.pop('cast_name')),\n",
    "                          #columns=mlb.classes_,\n",
    "                          #index=all_data_1.index))\n",
    "    \n",
    "    castname_dummy = pd.DataFrame(mlb.fit_transform(all_data_3['cast_name']),\n",
    "                               columns=mlb.classes_, index=all_data_3.index)\n",
    "  \n",
    "    return castname_dummy\n",
    "\n",
    "castname_dummy = process_cast_name(all_data_3['cast'])\n",
    "\n",
    "# select top 40 the most frequent cast names\n",
    "castname_dummy_reduce = castname_dummy.loc[:, (castname_dummy.sum(axis=0) > 40)]\n",
    "\n",
    "all_data_3 = pd.concat([all_data_3, castname_dummy_reduce], axis = 1)\n",
    "all_data_3 = all_data_3.drop(['cast'], axis = 1)\n",
    "\n",
    "# Preprocess feature - crew\n",
    "import ast\n",
    "def parse_crew(x):\n",
    "    try:\n",
    "        results = ast.literal_eval(x)\n",
    "    except:\n",
    "        results = []\n",
    "    return results\n",
    "\n",
    "def process_crews(variable):\n",
    "    \n",
    "    num_null = variable.isnull().sum()\n",
    "    \n",
    "    if num_null > 0:\n",
    "        variable = variable.fillna('NaN')\n",
    "    \n",
    "    else: \n",
    "        variable = variable\n",
    "        \n",
    "    crews = variable.map(lambda x: parse_crew(x))\n",
    "    \n",
    "    all_data_3['genders_0_crew'] = crews.apply(lambda x: sum([1 for i in x if i['gender'] == 0]))\n",
    "    all_data_3['genders_1_crew'] = crews.apply(lambda x: sum([1 for i in x if i['gender'] == 1]))\n",
    "    all_data_3['genders_2_crew'] = crews.apply(lambda x: sum([1 for i in x if i['gender'] == 2]))\n",
    "    \n",
    "    all_data_3['crew_count'] = crews.apply(lambda x : len(x))\n",
    "    \n",
    "    return all_data_3['crew_count'], all_data_3['genders_0_crew'], \\\n",
    "           all_data_3['genders_1_crew'], all_data_3['genders_2_crew']\n",
    "\n",
    "all_data_3['crew_count'], all_data_3['genders_0_crew'], \\\n",
    "           all_data_3['genders_1_crew'], all_data_3['genders_2_crew'] = process_crews(all_data_3['crews'])\n",
    "    \n",
    "all_data_3 = all_data_3.drop(['crews'], axis=1)\n",
    "\n",
    "# Preprocess feature - status\n",
    "def process_status(variable):\n",
    "    \n",
    "    all_data_3['isMovieReleased'] = 1\n",
    "    all_data_3.loc[variable != \"Released\" ,\"isMovieReleased\"] = 0 \n",
    "    \n",
    "    return all_data_3['isMovieReleased']\n",
    "\n",
    "all_data_3['isMovieReleased'] = process_status(all_data_3['status'])\n",
    "\n",
    "all_data_3 = all_data_3.drop(['status'], axis=1)\n",
    "\n",
    "#check on variables with infinite value\n",
    "for col in all_data_3.columns:\n",
    "    if not np.all(np.isfinite(all_data_3[col].values)):\n",
    "        print(col)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
